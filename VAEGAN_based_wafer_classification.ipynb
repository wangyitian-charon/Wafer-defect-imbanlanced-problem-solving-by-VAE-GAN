{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_three_chanel(wafer):\n",
    "    new_x = np.zeros((len(wafer), 64, 64, 3))\n",
    "\n",
    "    for w in range(len(wafer)):\n",
    "        for i in range(64):\n",
    "            for j in range(64):\n",
    "                new_x[w, i, j, int(wafer[w, i, j])] = 1\n",
    "    return new_x\n",
    "\n",
    "label = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch =np.ones((1, 64, 64))\n",
    "for i in range(1192):\n",
    "    S_i = str(i)\n",
    "    img = np.load('Raw_data/Scratch/'+S_i+'.npy', encoding='bytes', allow_pickle=True)\n",
    "    scratch =np.concatenate((scratch,img.reshape(1,64, 64)))\n",
    "    label.append(6)\n",
    "scratch =scratch[1:]\n",
    "scratch=scratch.reshape(-1,64, 64,1)\n",
    "scratch=create_three_chanel(scratch)\n",
    "\n",
    "random=np.ones((1, 64, 64))\n",
    "for i in range(865):\n",
    "    S_i = str(i)\n",
    "    img = np.load('Raw_data/Random/'+S_i+'.npy', encoding='bytes', allow_pickle=True)\n",
    "    random =np.concatenate((random,img.reshape(1,64, 64)))\n",
    "    label.append(5)\n",
    "\n",
    "random =random[1:]\n",
    "random=random.reshape(-1,64, 64,1)\n",
    "random=create_three_chanel(random)\n",
    "\n",
    "none=np.ones((1, 64, 64))\n",
    "for i in range(5000):\n",
    "    S_i = str(i)\n",
    "    img = np.load('Raw_data/None/'+S_i+'.npy', encoding='bytes', allow_pickle=True)\n",
    "    none =np.concatenate((none,img.reshape(1,64, 64)))\n",
    "    label.append(7)\n",
    "\n",
    "none =none[1:]\n",
    "none=none.reshape(-1,64, 64,1)\n",
    "none=create_three_chanel(none)\n",
    "\n",
    "near_full=np.ones((1, 64, 64))\n",
    "for i in range(148):\n",
    "    S_i = str(i)\n",
    "    img = np.load('Raw_data/Near_full/'+S_i+'.npy')\n",
    "    near_full =np.concatenate((near_full,img.reshape(1,64, 64)))\n",
    "    label.append(8)\n",
    "\n",
    "near_full =near_full[1:]\n",
    "near_full=near_full.reshape(-1,64, 64,1)\n",
    "near_full=create_three_chanel(near_full)\n",
    "\n",
    "loc =np.ones((1, 64, 64))\n",
    "for i in range(3592):\n",
    "    S_i = str(i)\n",
    "    img = np.load('Raw_data/Loc/'+S_i+'.npy')\n",
    "    loc =np.concatenate((loc,img.reshape(1,64, 64)))\n",
    "    label.append(4)\n",
    "\n",
    "loc =loc[1:]\n",
    "loc =loc.reshape(-1,64, 64,1)\n",
    "loc=create_three_chanel(loc)\n",
    "\n",
    "edge_ring=np.ones((1, 64, 64))\n",
    "for i in range(8000):\n",
    "    S_i = str(i)\n",
    "    img = np.load('Raw_data/Edge_Ring/'+S_i+'.npy')\n",
    "    edge_ring =np.concatenate((edge_ring,img.reshape(1,64, 64)))\n",
    "    label.append(2)\n",
    "edge_ring =edge_ring[1:]\n",
    "edge_ring =edge_ring.reshape(-1,64, 64,1)\n",
    "edge_ring=create_three_chanel(edge_ring)\n",
    "\n",
    "donut=np.ones((1, 64, 64))\n",
    "for i in range(554):\n",
    "    S_i = str(i)\n",
    "    img = np.load('Raw_data/Donut/'+S_i+'.npy')\n",
    "    donut =np.concatenate((donut,img.reshape(1,64, 64)))\n",
    "    label.append(1)\n",
    "donut =donut[1:]\n",
    "donut =donut.reshape(-1,64, 64,1)\n",
    "donut=create_three_chanel(donut)\n",
    "\n",
    "edge_loc =np.ones((1, 64, 64))\n",
    "for i in range(5188):\n",
    "    S_i = str(i)\n",
    "    img = np.load('Raw_data/Edge_Loc/'+S_i+'.npy')\n",
    "    edge_loc = np.concatenate((edge_loc,img.reshape(1,64, 64)))\n",
    "    label.append(3)\n",
    "edge_loc =edge_loc[1:]\n",
    "edge_loc =edge_loc.reshape(-1,64, 64,1)\n",
    "edge_loc=create_three_chanel(edge_loc)\n",
    "\n",
    "center =np.ones((1, 64, 64))\n",
    "for i in range(4293):\n",
    "    S_i = str(i)\n",
    "    img = np.load('Raw_data/Center/'+S_i+'.npy')\n",
    "    center = np.concatenate((center,img.reshape(1,64, 64)))\n",
    "    label.append(0)\n",
    "center = center[1:]\n",
    "center = center.reshape(-1,64, 64,1)\n",
    "center = create_three_chanel(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.concatenate((scratch,random,none,near_full,loc,edge_ring,donut,edge_loc,center))\n",
    "y= np.array(label)\n",
    "x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.2,\n",
    "                                                    random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "p_o = open('./x_test.pickle','wb')\n",
    "pickle.dump(x_test, p_o)\n",
    "p_o.close()\n",
    "\n",
    "p_o = open('./y_test.pickle','wb')\n",
    "pickle.dump(y_test, p_o)\n",
    "p_o.close()\n",
    "\n",
    "p_o = open('./x_train.pickle','wb')\n",
    "pickle.dump(x_train, p_o)\n",
    "p_o.close()\n",
    "\n",
    "p_o = open('./y_train.pickle','wb')\n",
    "pickle.dump(y_train, p_o)\n",
    "p_o.close()\n",
    "\n",
    "\n",
    "#######\n",
    "p_o = open('./center.pickle','wb')\n",
    "pickle.dump(center, p_o)\n",
    "p_o.close()\n",
    "\n",
    "p_o = open('./donut.pickle','wb')\n",
    "pickle.dump(donut, p_o)\n",
    "p_o.close()\n",
    "\n",
    "p_o = open('./random.pickle','wb')\n",
    "pickle.dump(random, p_o)\n",
    "p_o.close()\n",
    "\n",
    "p_o = open('./none.pickle','wb')\n",
    "pickle.dump(none, p_o)\n",
    "p_o.close()\n",
    "\n",
    "p_o = open('./near_full.pickle','wb')\n",
    "pickle.dump(near_full, p_o)\n",
    "p_o.close()\n",
    "\n",
    "p_o = open('./loc.pickle','wb')\n",
    "pickle.dump(loc, p_o)\n",
    "p_o.close()\n",
    "\n",
    "p_o = open('./edge_loc.pickle','wb')\n",
    "pickle.dump(edge_loc, p_o)\n",
    "p_o.close()\n",
    "\n",
    "p_o = open('./scratch.pickle','wb')\n",
    "pickle.dump(scratch, p_o)\n",
    "p_o.close()\n",
    "\n",
    "p_o = open('./edge_ring.pickle','wb')\n",
    "pickle.dump(edge_ring, p_o)\n",
    "p_o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Convolution2D, MaxPooling2D, UpSampling2D ,Conv2DTranspose,Conv2D,Dense,Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras.datasets import mnist  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import LeakyReLU,Flatten\n",
    "from keras import models,layers\n",
    "import pickle\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = open('./center.pickle','rb')\n",
    "center = pickle.load(data_in)\n",
    "\n",
    "data_in = open('./donut.pickle','rb')\n",
    "donut = pickle.load(data_in)\n",
    "\n",
    "data_in = open('./random.pickle','rb')\n",
    "radnom = pickle.load(data_in)\n",
    "\n",
    "data_in = open('./scratch.pickle','rb')\n",
    "scratch = pickle.load(data_in)\n",
    "\n",
    "data_in = open('./edge_loc.pickle','rb')\n",
    "edge_loc = pickle.load(data_in)\n",
    "\n",
    "data_in = open('./edge_ring.pickle','rb')\n",
    "edge_ring = pickle.load(data_in)\n",
    "\n",
    "data_in = open('./loc.pickle','rb')\n",
    "loc = pickle.load(data_in)\n",
    "\n",
    "data_in = open('./none.pickle','rb')\n",
    "none = pickle.load(data_in)\n",
    "\n",
    "data_in = open('./near_full.pickle','rb')\n",
    "near_full = pickle.load(data_in)\n",
    "\n",
    "data_in = open('./x_train.pickle','rb')\n",
    "x_train = pickle.load(data_in)\n",
    "\n",
    "data_in = open('./x_test.pickle','rb')\n",
    "x_test = pickle.load(data_in)\n",
    "\n",
    "data_in = open('./y_train.pickle','rb')\n",
    "y_train = pickle.load(data_in)\n",
    "\n",
    "data_in = open('./y_test.pickle','rb')\n",
    "y_test = pickle.load(data_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(center[21],cmap =plt.get_cmap('Blues'))\n",
    "plt.show()\n",
    "plt.imshow(center[55], interpolation='nearest', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(16,16,512), mean=0.,\n",
    "                              stddev=1)    \n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "input_shape = (64, 64, 3)\n",
    "input_tensor = Input(input_shape)\n",
    "encode = layers.Conv2D(128, (3,3), padding='same', activation='relu')(input_tensor)\n",
    "encode = layers.MaxPooling2D()(encode)\n",
    "encode = layers.Conv2D(256, (3,3), padding='same', activation='relu')(encode)\n",
    "encode = layers.MaxPooling2D()(encode)\n",
    "#encode = layers.Conv2D(512, (3,3), padding='same', activation='relu')(encode)\n",
    "z_mean = layers.Conv2D(512, (3,3), padding='same', activation='relu')(encode)\n",
    "\n",
    "#latent_vector =layers.Dense(1024)()\n",
    "z_log_var  = layers.Conv2D(512,(3,3), padding='same', activation='relu')(encode)\n",
    "z = layers.Lambda(sampling, output_shape=(16,16,512))([z_mean, z_log_var])\n",
    "# Decoder\n",
    "decode1 = layers.Conv2DTranspose(512, (3,3), padding='same', activation='relu')\n",
    "decode2 = layers.UpSampling2D()\n",
    "decode3 = layers.Conv2DTranspose(256, (3,3), padding='same', activation='relu')\n",
    "decode4 = layers.UpSampling2D()\n",
    "decode5 = layers.Conv2DTranspose(128, (3,3), padding='same', activation='relu')\n",
    "output_tensor = Conv2DTranspose(3, (3,3), padding='same', activation='sigmoid')\n",
    "decode = decode1(z)\n",
    "decode = decode2(decode) \n",
    "decode = decode3(decode) \n",
    "decode = decode4(decode) \n",
    "decode = decode5(decode)\n",
    "\n",
    "ae = models.Model(input_tensor, output_tensor(decode))\n",
    "ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###discriminator\n",
    "def discriminator_bulider():\n",
    "  \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(64,64,3)))  \n",
    "    #model.add(Dense(1024))\n",
    "    #model.add(LeakyReLU(alpha=0.2))\n",
    "    #model.add(Dense(512))\n",
    "    #model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(128))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))  \n",
    "   \n",
    "    model.summary()\n",
    "    img = Input(shape=(64,64,3))\n",
    "    validity = model(img)  \n",
    "\n",
    "    return  Model(img, validity) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = discriminator_bulider()\n",
    "ae.compile(optimizer='adam', loss='MSE')  \n",
    "discriminator.compile(optimizer='adam',loss='binary_crossentropy')\n",
    "ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "#train_model\n",
    "batch_size = 128\n",
    "epochs = 300\n",
    "\n",
    "\n",
    "\n",
    "valid = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))\n",
    "for epoch in range (epochs):\n",
    "    # \n",
    "    if epoch  <200: #对前100个epoch做判别器训练\n",
    "        \n",
    "        \n",
    "        idx_d = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "        imgs_d = x_train[idx_d]\n",
    "        idx_g= np.random.randint(0, x_train.shape[0], batch_size)\n",
    "        imgs_g = x_train[idx_g]##随机从训练数据从选出batchsize个，作为输入\n",
    "\n",
    "\n",
    "        gen_imgs_d = ae.predict(imgs_d)#用输入图像创造生成图像，既fake图像\n",
    "        d_loss_real = discriminator.train_on_batch(imgs_d, valid) #图像为真的loss\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs_d, fake) #图像为假的loss\n",
    "        d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "\n",
    "    \n",
    "    g_loss = ae.train_on_batch(imgs_g,imgs_g)\n",
    "\n",
    "\n",
    "\n",
    "    print('epoch:%d ;dloss %s ;gloss %s' %(epoch,d_loss,g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = models.Model(input_tensor, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input((16, 16, 512))\n",
    "\n",
    "decode = decode1(decoder_input)\n",
    "decode = decode2(decode) \n",
    "decode = decode3(decode) \n",
    "decode = decode4(decode) \n",
    "decode = decode5(decode) \n",
    "decoder = models.Model(decoder_input, output_tensor(decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = encoder.predict(center)\n",
    "n=0\n",
    "noised_x = a  #+ np.random.normal(loc=0, scale=0.005, size = (len(center),16,16,512)) \n",
    "noised_gen_x = decoder.predict(noised_x)\n",
    "vali = discriminator.predict(noised_gen_x)\n",
    "for i in range (len(center)):\n",
    "    if vali[i]>0.5:\n",
    "        n= n+1\n",
    "    \n",
    "print(n)\n",
    "print(len(center))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(noised_gen_x[345])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(img,label,t=0.5):\n",
    "    # dummy array for collecting noised wafer\n",
    "    gen_x = np.zeros((1, 64, 64, 3))                                                      ##########\n",
    "    sorted_x =np.zeros((1, 64, 64, 3))\n",
    "    # Make wafer until total # of wafer to 2000\n",
    "    n=0\n",
    "        \n",
    "    while True:\n",
    "        n=n+1\n",
    "        id = np.random.randint(0, img.shape[0], 1)\n",
    "        imgs = img[id].reshape(-1,64,64,3)\n",
    "        encoded_x = encoder.predict(imgs)\n",
    "        noised_x = encoded_x  #+ np.random.normal(loc=0, scale=0.005, size = (1,16,16,512)) \n",
    "        noised_gen_x = decoder.predict(noised_x)\n",
    "        validity = discriminator.predict(noised_gen_x)\n",
    "        if validity >t:\n",
    "            gen_x = np.concatenate((gen_x,noised_gen_x))       \n",
    "        if len(gen_x)>3000:\n",
    "            break\n",
    "        if n >8000:\n",
    "            break\n",
    "\n",
    "\n",
    "    # also make label vector with same length\n",
    "    \n",
    "    gen_y = np.full((len(gen_x), 1), label)\n",
    "    \n",
    "     #return date without 1st dummy data.'''\n",
    "    return gen_x[1:],gen_y[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_center,label_center=gen_data(center,0)\n",
    "new_donut,label_donut=gen_data(donut,1)\n",
    "new_ering,label_ering=gen_data(edge_ring,2)\n",
    "new_eloc,label_eloc=gen_data(edge_loc,3)\n",
    "new_loc,label_loc=gen_data(loc,4)\n",
    "new_random,label_random=gen_data(radnom,5)\n",
    "new_scratch,label_scratch=gen_data(scratch,6)\n",
    "new_none,label_none=gen_data(none,7)\n",
    "new_nfull,label_nfull=gen_data(near_full,8)\n",
    "#sorted_center,=sort_data(new_center,'center',0.5)\n",
    "print(new_center.shape)\n",
    "print(new_donut.shape)\n",
    "print(new_ering.shape)\n",
    "print(new_eloc.shape)\n",
    "print(new_loc.shape)\n",
    "print(new_random.shape)\n",
    "print(new_scratch.shape)\n",
    "print(new_none.shape)\n",
    "print(new_nfull.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "train_data= np.concatenate((new_center,new_eloc,new_donut,new_ering,new_loc,new_nfull,new_none,new_random,new_scratch))\n",
    "label_data= np.concatenate((label_center,label_eloc,label_donut,label_ering,label_loc,label_nfull,label_none,label_random,label_scratch))\n",
    "\n",
    "p_o = open('./generateX.pickle','wb')\n",
    "pickle.dump(train_data, p_o)\n",
    "p_o.close()\n",
    "\n",
    "p_o = open('./generateY.pickle','wb')\n",
    "pickle.dump(label_data, p_o)\n",
    "p_o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_data= np.concatenate((train_data,x_train))\n",
    "y_train =y_train.reshape(-1,1)\n",
    "label_data= np.concatenate((label_data,y_train))\n",
    "#label_data = to_categorical(label_data)\n",
    "#y_test = to_categorical(y_test)\n",
    "print(x_train.shape)\n",
    "print(train_data.shape)\n",
    "print(label_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train= to_categorical(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Activation, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64,(3,3),input_shape =(64,64,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "                        padding='same')\n",
    "                        )\n",
    "model.add(Conv2D(128,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "                        padding='same')\n",
    "                        )                    \n",
    "model.add(Conv2D(256,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "                        padding='same')\n",
    "                        )\n",
    "model.add(Conv2D(512,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "                        padding='same')\n",
    "                        )     \n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(9))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=1024, epochs=4, validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
